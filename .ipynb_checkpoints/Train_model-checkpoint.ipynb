{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import yaml, cv2, os\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import model_add as model\n",
    "import model_concat as mconcat\n",
    "import model_utility as ut\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets with ImageFolder\n",
    "\n",
    "image_dir_8 = 'resolution_dataset/lr_f1_160_8'\n",
    "label_dir = 'resolution_dataset/hr_f1_160'\n",
    "\n",
    "dir_dictionary = ut.create_dictionary(image_dir_8,label_dir)\n",
    "\n",
    "trans = transforms.Compose([\n",
    "   transforms.CenterCrop(400),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = ut.MRIDataset(image_dir_8,label_dir,dir_dict = dir_dictionary,transform=trans,test=False)\n",
    "\n",
    "# Using the image datasets and the transforms, define the dataloaders\n",
    "# dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=32, shuffle=True, num_workers=2)\n",
    "dataloaders = DataLoader(image_datasets, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/cidar/Documents/MRI_Super_resolution/Scripts/model_utility.py\", line 42, in __getitem__\n    image = self.transform(image)\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n    img = t(img)\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 235, in __call__\n    return F.center_crop(img, self.size)\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 365, in center_crop\n    w, h = img.size\nTypeError: cannot unpack non-iterable builtin_function_or_method object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_294063/1839087850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert images to numpy for display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plot the images in the batch, along with the corresponding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/cidar/Documents/MRI_Super_resolution/Scripts/model_utility.py\", line 42, in __getitem__\n    image = self.transform(image)\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n    img = t(img)\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 235, in __call__\n    return F.center_crop(img, self.size)\n  File \"/home/cidar/anaconda3/envs/resolution_env/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 365, in center_crop\n    w, h = img.size\nTypeError: cannot unpack non-iterable builtin_function_or_method object\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(dataloaders)\n",
    "images, labels, parsers = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(13,25))\n",
    "k=0\n",
    "for idx in np.arange(0,15,3):\n",
    "    ax = fig.add_subplot(15/3, 3, idx+1, xticks=[], yticks=[])\n",
    "    ut.imshow(images[k].squeeze(),'image:'+str(k))\n",
    "    ax = fig.add_subplot( 15/3,3, idx+2, xticks=[], yticks=[])\n",
    "    ut.imshow(labels[k].squeeze(),'label:'+str(k))\n",
    "    ax = fig.add_subplot(15/3,3, idx+3, xticks=[], yticks=[])\n",
    "    ut.imshow(parsers[k].squeeze()*10 ,'parsers:'+str(k))\n",
    "    k +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "channels = 1\n",
    "n_epochs = 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "lbda = 0.3\n",
    "alpha = 0.5\n",
    "beta = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a concatenation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_concat = mconcat.MRINet(device)\n",
    "model_concat = nn.DataParallel(model_concat,device_ids=[0,1])\n",
    "model_concat.to(device)\n",
    "# print(model_concat)\n",
    "\n",
    "# tb = SummaryWriter()\n",
    "\n",
    "\n",
    "# images, labels,parsers = next(iter(dataloaders))\n",
    "# grid = torchvision.utils.make_grid(images)\n",
    "# # grid_labels = torchvision.utils.make_grid(labels)\n",
    "# # # grid_parsers= torchvision.utils.make_grid(parsers)\n",
    "# tb.add_image(\"images\", grid)\n",
    "# tb.add_image(\"labels\", grid_labels)\n",
    "# tb.add_image(\"parsers\", grid_parsers)\n",
    "# tb.close()\n",
    "\n",
    "# model_concat.eval()\n",
    "# with torch.no_grad():\n",
    "#     tb.add_graph(model_concat,images)\n",
    "\n",
    "optim = torch.optim.SGD(params = model_concat.parameters(), lr = learning_rate, momentum=0.9)\n",
    "\n",
    "model_concat.train() \n",
    "losses_concat = []\n",
    "k = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for idx, (images, labels,parsers) in enumerate(dataloaders):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        parsers = parsers.to(device)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        outputs,f,p,y_c = model_concat(images)\n",
    "\n",
    "        loss_coarser = loss_fn(y_c, labels) \n",
    "        loss_decoder = loss_fn(outputs, labels) \n",
    "        loss_parser = loss_fn(p, parsers) \n",
    "        loss = (loss_coarser*beta) + (alpha * loss_decoder) + (loss_parser * lbda)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step() \n",
    "\n",
    "        losses_concat.append(float(loss))\n",
    "#         tb.add_scalar(\"Loss\", float(loss), idx)\n",
    "        \n",
    "        if idx % 200 == 0:\n",
    "            print(\"Epoch [%d/%d]. Iter [%d/%d]. Loss: %0.2f\" % (epoch, n_epochs, idx + 1, len(dataloaders), loss))\n",
    "#     for name, weight in model_concat.named_parameters():\n",
    "#         tb.add_histogram(name, weight, epoch)\n",
    "#         tb.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
    "#     tb.add_scalar(\"Loss\", float(loss), epoch)\n",
    "    if k == 100:\n",
    "        path = f'model/concat/model_concat_'+str(epoch)+'.pth'\n",
    "        torch.save(model_concat.state_dict(), path)\n",
    "        k=0\n",
    "    k+=1\n",
    "# tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'model/concat/model_concat'+'_final.pth'\n",
    "torch.save(model_concat.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Addition Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_add = model.MRINet(device)\n",
    "model_add = nn.DataParallel(model_add,device_ids=[0,1])\n",
    "model_add.to(device)\n",
    "# print(model_add)\n",
    "\n",
    "optim = torch.optim.SGD(params = model_add.parameters(), lr = learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "model_add.train() \n",
    "losses_add = []\n",
    "k = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for idx, (images, labels,parsers) in enumerate(dataloaders):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        parsers = parsers.to(device)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        outputs,f,p,y_c = model_add(images)\n",
    "\n",
    "        loss_coarser = loss_fn(y_c, labels) \n",
    "        loss_decoder = loss_fn(outputs, labels) \n",
    "        loss_parser = loss_fn(p, parsers) \n",
    "        loss = (loss_coarser*beta) + (alpha * loss_decoder) + (loss_parser * lbda)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step() \n",
    "\n",
    "\n",
    "        losses_add.append(float(loss))\n",
    "        \n",
    "        if idx % 200 == 0:\n",
    "            print(\"Epoch [%d/%d]. Iter [%d/%d]. Loss: %0.2f\" % (epoch, n_epochs, idx + 1, len(dataloaders), loss))\n",
    "    if k == 100:\n",
    "        path = f'model/add/model_add_'+str(epoch)+'.pth'\n",
    "        torch.save(model_add.state_dict(), path)\n",
    "        k=0\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'model/add/model_add'+'_final.pth'\n",
    "torch.save(model_add.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses_concat)\n",
    "plt.title('loss plot for concatenation network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses_add)\n",
    "plt.title('loss plot for additive network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
